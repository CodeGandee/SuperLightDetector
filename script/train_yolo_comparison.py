from ultralytics import YOLO
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import pandas as pd
import numpy as np
from pathlib import Path
import os
import logging
import sys
from datetime import datetime
import json

# è®¾ç½®æ—¥å¿—è®°å½•
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
    log_dir = Path("label_data_PR/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºlogger
    logger = logging.getLogger('yolo_comparison')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(
        log_dir / f"yolo_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logging()

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = Path("label_data_PR")
output_dir.mkdir(exist_ok=True)

# å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹ç‰ˆæœ¬ï¼ˆæŒ‰å¯ç”¨æ€§æ’åºï¼‰
model_versions = [
    "yolo12n",    # YOLO12n
    "yolo11n",  # YOLOv11n 
    "yolov10n",  # YOLOv10n
    "yolov9t",   # YOLOv9t
    "yolov8n",   # YOLOv8n (æœ€ç¨³å®š)
    "yolov5nu",  # YOLOv5n ultralyticsç‰ˆæœ¬
]

# å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
all_results = []

def log_and_print(message):
    """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œè®°å½•åˆ°æ—¥å¿—"""
    logger.info(message)
    print(message)

def check_model_availability(model_name):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        model = YOLO(f"{model_name}.pt")
        return True
    except Exception as e:
        log_and_print(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨: {str(e)}")
        return False

def train_and_evaluate_model(model_name):
    """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
    log_and_print(f"\n{'='*60}")
    log_and_print(f"å¼€å§‹å¤„ç†æ¨¡å‹: {model_name}")
    log_and_print(f"{'='*60}")
    
    try:
        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        if not check_model_availability(model_name):
            return {
                'model': model_name,
                'val_mAP50': 0,
                'val_mAP50_95': 0,
                'val_precision': 0,
                'val_recall': 0,
                'test_mAP50': 0,
                'test_mAP50_95': 0,
                'test_precision': 0,
                'test_recall': 0,
                'status': 'unavailable',
                'error': 'Model not available'
            }
        
        # åŠ è½½æ¨¡å‹
        model = YOLO(f"{model_name}.pt")
        log_and_print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}.pt")
        
        # è®­ç»ƒæ¨¡å‹
        log_and_print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {model_name}...")
        train_results = model.train(
            data="label_data.yaml",
            epochs=200,  # å‡å°‘è½®æ•°ä»¥å¿«é€Ÿæ¯”è¾ƒ
            imgsz=640,
            device="2,3",
            batch=4,
            lr0=0.001,
            lrf=0.1,
            momentum=0.9,
            weight_decay=0.0005,
            warmup_epochs=3,
            warmup_momentum=0.8,
            warmup_bias_lr=0.1,
            box=7.5,
            cls=0.5,
            dfl=1.5,
            val=True,
            save=True,
            plots=True,  # å…³é—­è®­ç»ƒå›¾è¡¨ä»¥èŠ‚çœæ—¶é—´
            verbose=False,
            project="label_data_PR",
            name=f"{model_name}_comparison_200",
            exist_ok=True,
            pretrained=True,
            optimizer='AdamW',
            close_mosaic=10,
            resume=False,
            amp=False,
            fraction=1.0,
            profile=False,
            # æ•°æ®å¢å¼ºå‚æ•°
            hsv_h=0.015,
            hsv_s=0.7,
            hsv_v=0.4,
            degrees=0.0,
            translate=0.1,
            scale=0.5,
            shear=0.0,
            perspective=0.0,
            flipud=0.0,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.0,
            copy_paste=0.0,
        )
        
        log_and_print(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ")
        
        # éªŒè¯é›†è¯„ä¼°
        log_and_print(f"ğŸ“Š å¼€å§‹éªŒè¯é›†è¯„ä¼° {model_name}...")
        val_metrics = model.val(
            data="label_data.yaml",
            split='val',
            plots=False,  # å…³é—­éªŒè¯å›¾è¡¨
            save_json=False,
            verbose=False
        )
        
        # æµ‹è¯•é›†è¯„ä¼°
        log_and_print(f"ğŸ”¬ å¼€å§‹æµ‹è¯•é›†è¯„ä¼° {model_name}...")
        test_metrics = model.val(
            data="label_data.yaml",
            split='test',
            plots=False,
            save_json=False,
            verbose=False
        )
        
        # æ”¶é›†ç»“æœ
        result = {
            'model': model_name,
            'val_mAP50': float(val_metrics.box.map50),
            'val_mAP50_95': float(val_metrics.box.map),
            'val_precision': float(val_metrics.box.mp),
            'val_recall': float(val_metrics.box.mr),
            'test_mAP50': float(test_metrics.box.map50),
            'test_mAP50_95': float(test_metrics.box.map),
            'test_precision': float(test_metrics.box.mp),
            'test_recall': float(test_metrics.box.mr),
            'status': 'success'
        }
        
        log_and_print(f"ğŸ“ˆ æ¨¡å‹ {model_name} è¯„ä¼°ç»“æœ:")
        log_and_print(f"   éªŒè¯é›† - mAP50: {val_metrics.box.map50:.4f}, mAP50-95: {val_metrics.box.map:.4f}")
        log_and_print(f"   éªŒè¯é›† - ç²¾ç¡®åº¦: {val_metrics.box.mp:.4f}, å¬å›ç‡: {val_metrics.box.mr:.4f}")
        log_and_print(f"   æµ‹è¯•é›† - mAP50: {test_metrics.box.map50:.4f}, mAP50-95: {test_metrics.box.map:.4f}")
        log_and_print(f"   æµ‹è¯•é›† - ç²¾ç¡®åº¦: {test_metrics.box.mp:.4f}, å¬å›ç‡: {test_metrics.box.mr:.4f}")
        
        return result
        
    except Exception as e:
        log_and_print(f"âŒ æ¨¡å‹ {model_name} å¤„ç†å¤±è´¥: {str(e)}")
        return {
            'model': model_name,
            'val_mAP50': 0,
            'val_mAP50_95': 0,
            'val_precision': 0,
            'val_recall': 0,
            'test_mAP50': 0,
            'test_mAP50_95': 0,
            'test_precision': 0,
            'test_recall': 0,
            'status': 'failed',
            'error': str(e)
        }

def save_results_to_csv(results, filename="model_comparison_results.csv"):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    df = pd.DataFrame(results)
    csv_path = output_dir / filename
    df.to_csv(csv_path, index=False)
    log_and_print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°CSV: {csv_path}")
    return df

def create_comparison_charts(df):
    """åˆ›å»ºæ¨¡å‹æ¯”è¾ƒå›¾è¡¨"""
    # è®¾ç½®matplotlibå­—ä½“
    try:
        # å°è¯•å¯»æ‰¾ä¸­æ–‡å­—ä½“
        font_paths = [
            'SimSun.ttf',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            '/System/Library/Fonts/Arial.ttf',
            '/usr/share/fonts/TTF/SimSun.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'
        ]
        
        chinese_font = None
        for font_path in font_paths:
            if os.path.exists(font_path):
                chinese_font = fm.FontProperties(fname=font_path)
                break
        
        if chinese_font is None:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå­—ä½“ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“
            chinese_font = fm.FontProperties(family='sans-serif')
            
    except Exception as e:
        log_and_print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“: {e}")
        chinese_font = fm.FontProperties(family='sans-serif')
    
    # åªæ˜¾ç¤ºæˆåŠŸçš„æ¨¡å‹
    df_success = df[df['status'] == 'success'].copy()
    
    if len(df_success) == 0:
        log_and_print("âš ï¸  æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹ç»“æœï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
        return
    
    log_and_print(f"ğŸ“Š ç”Ÿæˆ {len(df_success)} ä¸ªæˆåŠŸæ¨¡å‹çš„æ¯”è¾ƒå›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle('YOLO Model Performance Comparison', fontsize=18, fontweight='bold', y=0.95, fontproperties=chinese_font)
    
    models = df_success['model'].tolist()
    colors = plt.cm.Set1(np.linspace(0, 1, len(models)))
    
    # 1. mAP50æ¯”è¾ƒ
    x_pos = np.arange(len(models))
    width = 0.35
    axes[0, 0].bar(x_pos - width/2, df_success['val_mAP50'], width, 
                   label='Validation', color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
    axes[0, 0].bar(x_pos + width/2, df_success['test_mAP50'], width, 
                   label='Test', color=colors, alpha=0.6, edgecolor='black', linewidth=0.5)
    axes[0, 0].set_title('mAP@0.5 Performance Comparison', fontsize=14, fontweight='bold', fontproperties=chinese_font)
    axes[0, 0].set_ylabel('mAP@0.5', fontsize=12, fontproperties=chinese_font)
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels(models, rotation=15, ha='right', fontproperties=chinese_font)
    axes[0, 0].legend(fontsize=11, prop=chinese_font)
    axes[0, 0].grid(True, alpha=0.3, linestyle='--')
    axes[0, 0].set_ylim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (val_score, test_score) in enumerate(zip(df_success['val_mAP50'], df_success['test_mAP50'])):
        axes[0, 0].text(i-width/2, val_score + 0.02, f'{val_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
        axes[0, 0].text(i+width/2, test_score + 0.02, f'{test_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
    
    # 2. mAP50-95æ¯”è¾ƒ
    axes[0, 1].bar(x_pos - width/2, df_success['val_mAP50_95'], width, 
                   label='Validation', color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
    axes[0, 1].bar(x_pos + width/2, df_success['test_mAP50_95'], width, 
                   label='Test', color=colors, alpha=0.6, edgecolor='black', linewidth=0.5)
    axes[0, 1].set_title('mAP@0.5:0.95 Performance Comparison', fontsize=14, fontweight='bold', fontproperties=chinese_font)
    axes[0, 1].set_ylabel('mAP@0.5:0.95', fontsize=12, fontproperties=chinese_font)
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels(models, rotation=15, ha='right', fontproperties=chinese_font)
    axes[0, 1].legend(fontsize=11, prop=chinese_font)
    axes[0, 1].grid(True, alpha=0.3, linestyle='--')
    axes[0, 1].set_ylim(0, 1.0)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (val_score, test_score) in enumerate(zip(df_success['val_mAP50_95'], df_success['test_mAP50_95'])):
        axes[0, 1].text(i-width/2, val_score + 0.02, f'{val_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
        axes[0, 1].text(i+width/2, test_score + 0.02, f'{test_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
    
    # 3. ç²¾ç¡®åº¦æ¯”è¾ƒ
    axes[1, 0].bar(x_pos - width/2, df_success['val_precision'], width, 
                   label='Validation', color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
    axes[1, 0].bar(x_pos + width/2, df_success['test_precision'], width, 
                   label='Test', color=colors, alpha=0.6, edgecolor='black', linewidth=0.5)
    axes[1, 0].set_title('Precision Performance Comparison', fontsize=14, fontweight='bold', fontproperties=chinese_font)
    axes[1, 0].set_ylabel('Precision', fontsize=12, fontproperties=chinese_font)
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels(models, rotation=15, ha='right', fontproperties=chinese_font)
    axes[1, 0].legend(fontsize=11, prop=chinese_font)
    axes[1, 0].grid(True, alpha=0.3, linestyle='--')
    axes[1, 0].set_ylim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (val_score, test_score) in enumerate(zip(df_success['val_precision'], df_success['test_precision'])):
        axes[1, 0].text(i-width/2, val_score + 0.02, f'{val_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
        axes[1, 0].text(i+width/2, test_score + 0.02, f'{test_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
    
    # 4. å¬å›ç‡æ¯”è¾ƒ
    axes[1, 1].bar(x_pos - width/2, df_success['val_recall'], width, 
                   label='Validation', color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
    axes[1, 1].bar(x_pos + width/2, df_success['test_recall'], width, 
                   label='Test', color=colors, alpha=0.6, edgecolor='black', linewidth=0.5)
    axes[1, 1].set_title('Recall Performance Comparison', fontsize=14, fontweight='bold', fontproperties=chinese_font)
    axes[1, 1].set_ylabel('Recall', fontsize=12, fontproperties=chinese_font)
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(models, rotation=15, ha='right', fontproperties=chinese_font)
    axes[1, 1].legend(fontsize=11, prop=chinese_font)
    axes[1, 1].grid(True, alpha=0.3, linestyle='--')
    axes[1, 1].set_ylim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (val_score, test_score) in enumerate(zip(df_success['val_recall'], df_success['test_recall'])):
        axes[1, 1].text(i-width/2, val_score + 0.02, f'{val_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
        axes[1, 1].text(i+width/2, test_score + 0.02, f'{test_score:.3f}', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold', fontproperties=chinese_font)
    
    plt.tight_layout()
    chart_path = output_dir / "model_comparison_charts.png"
    plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
    log_and_print(f"ğŸ“Š æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    
    # åˆ›å»ºé›·è¾¾å›¾
    create_radar_chart(df_success)

def create_radar_chart(df_success):
    """åˆ›å»ºé›·è¾¾å›¾æ˜¾ç¤ºæ¨¡å‹ç»¼åˆæ€§èƒ½"""
    if len(df_success) == 0:
        return
    
    # è®¾ç½®matplotlibå­—ä½“
    try:
        # å°è¯•å¯»æ‰¾ä¸­æ–‡å­—ä½“
        font_paths = [
            'SimSun.ttf',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            '/System/Library/Fonts/Arial.ttf',
            '/usr/share/fonts/TTF/SimSun.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'
        ]
        
        chinese_font = None
        for font_path in font_paths:
            if os.path.exists(font_path):
                chinese_font = fm.FontProperties(fname=font_path)
                break
        
        if chinese_font is None:
            chinese_font = fm.FontProperties(family='sans-serif')
            
    except Exception as e:
        log_and_print(f"âš ï¸ é›·è¾¾å›¾å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“: {e}")
        chinese_font = fm.FontProperties(family='sans-serif')
        
    log_and_print("ğŸ¯ ç”Ÿæˆæ¨¡å‹æ€§èƒ½é›·è¾¾å›¾...")
    
    # å‡†å¤‡é›·è¾¾å›¾æ•°æ® - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…å­—ä½“é—®é¢˜
    categories = ['Val mAP50', 'Val mAP50-95', 'Val Precision', 'Val Recall', 
                  'Test mAP50', 'Test mAP50-95', 'Test Precision', 'Test Recall']
    
    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    colors = plt.cm.Set1(np.linspace(0, 1, len(df_success)))
    
    for idx, (_, row) in enumerate(df_success.iterrows()):
        values = [
            row['val_mAP50'], row['val_mAP50_95'], row['val_precision'], row['val_recall'],
            row['test_mAP50'], row['test_mAP50_95'], row['test_precision'], row['test_recall']
        ]
        values += values[:1]  # é—­åˆ
        
        ax.plot(angles, values, 'o-', linewidth=2, label=row['model'], color=colors[idx])
        ax.fill(angles, values, alpha=0.25, color=colors[idx])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, fontsize=11, fontproperties=chinese_font)
    ax.set_ylim(0, 1)
    ax.set_title('YOLO Model Performance Radar Chart', fontsize=16, fontweight='bold', pad=20, fontproperties=chinese_font)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0), fontsize=11, prop=chinese_font)
    ax.grid(True)
    
    radar_path = output_dir / "model_performance_radar.png"
    plt.savefig(radar_path, dpi=300, bbox_inches='tight', facecolor='white')
    log_and_print(f"ğŸ¯ é›·è¾¾å›¾å·²ä¿å­˜åˆ°: {radar_path}")

def create_summary_table(df):
    """åˆ›å»ºè¯¦ç»†çš„æ±‡æ€»è¡¨æ ¼"""
    log_and_print("\n" + "="*100)
    log_and_print("ğŸ† YOLOæ¨¡å‹æ€§èƒ½è¯¦ç»†æ±‡æ€»è¡¨")
    log_and_print("="*100)
    
    # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çŠ¶æ€
    log_and_print("\nğŸ“‹ æ¨¡å‹çŠ¶æ€æ€»è§ˆ:")
    log_and_print("-" * 50)
    for _, row in df.iterrows():
        status_emoji = "âœ…" if row['status'] == 'success' else "âŒ" if row['status'] == 'failed' else "âš ï¸"
        log_and_print(f"{status_emoji} {row['model']:<15} - {row['status']}")
    
    # åªæ˜¾ç¤ºæˆåŠŸçš„æ¨¡å‹çš„è¯¦ç»†ç»“æœ
    df_success = df[df['status'] == 'success'].copy()
    
    if len(df_success) == 0:
        log_and_print("\nâŒ æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹ç»“æœ")
        return
    
    log_and_print(f"\nğŸ“Š æˆåŠŸè®­ç»ƒçš„æ¨¡å‹è¯¦ç»†æ€§èƒ½è¡¨ ({len(df_success)}ä¸ªæ¨¡å‹):")
    log_and_print("-" * 120)
    log_and_print(f"{'æ¨¡å‹':<12} {'éªŒè¯mAP50':<11} {'éªŒè¯mAP50-95':<13} {'éªŒè¯ç²¾ç¡®åº¦':<11} {'éªŒè¯å¬å›ç‡':<11} "
                  f"{'æµ‹è¯•mAP50':<11} {'æµ‹è¯•mAP50-95':<13} {'æµ‹è¯•ç²¾ç¡®åº¦':<11} {'æµ‹è¯•å¬å›ç‡':<11}")
    log_and_print("=" * 120)
    
    for _, row in df_success.iterrows():
        log_and_print(f"{row['model']:<12} {row['val_mAP50']:<11.4f} {row['val_mAP50_95']:<13.4f} "
                      f"{row['val_precision']:<11.4f} {row['val_recall']:<11.4f} "
                      f"{row['test_mAP50']:<11.4f} {row['test_mAP50_95']:<13.4f} "
                      f"{row['test_precision']:<11.4f} {row['test_recall']:<11.4f}")
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    log_and_print("-" * 120)
    avg_row = df_success.select_dtypes(include=[np.number]).mean()
    log_and_print(f"{'å¹³å‡å€¼':<12} {avg_row['val_mAP50']:<11.4f} {avg_row['val_mAP50_95']:<13.4f} "
                  f"{avg_row['val_precision']:<11.4f} {avg_row['val_recall']:<11.4f} "
                  f"{avg_row['test_mAP50']:<11.4f} {avg_row['test_mAP50_95']:<13.4f} "
                  f"{avg_row['test_precision']:<11.4f} {avg_row['test_recall']:<11.4f}")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    metrics = ['val_mAP50', 'test_mAP50', 'val_mAP50_95', 'test_mAP50_95']
    log_and_print("\nğŸ† å„é¡¹æŒ‡æ ‡æœ€ä½³æ¨¡å‹:")
    log_and_print("=" * 50)
    
    for metric in metrics:
        best_idx = df_success[metric].idxmax()
        best_model = df_success.loc[best_idx]
        metric_name = metric.replace('val_', 'éªŒè¯é›†').replace('test_', 'æµ‹è¯•é›†').replace('mAP50_95', 'mAP50-95')
        log_and_print(f"ğŸ¥‡ {metric_name:<15}: {best_model['model']:<12} ({best_model[metric]:.4f})")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    log_and_print("ğŸš€ å¼€å§‹YOLOæ¨¡å‹å…¨é¢æ¯”è¾ƒå®éªŒ")
    log_and_print(f"ğŸ“‹ è®¡åˆ’æµ‹è¯•çš„æ¨¡å‹: {model_versions}")
    log_and_print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: è®­ç»ƒé›†35å¼ , éªŒè¯é›†10å¼ , æµ‹è¯•é›†6å¼ ")
    log_and_print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    for i, model_name in enumerate(model_versions, 1):
        log_and_print(f"\nğŸ“… è¿›åº¦: {i}/{len(model_versions)} - å½“å‰æ¨¡å‹: {model_name}")
        result = train_and_evaluate_model(model_name)
        all_results.append(result)
    
    # ä¿å­˜ç»“æœåˆ°CSV
    df_results = save_results_to_csv(all_results)
    
    # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
    create_comparison_charts(df_results)
    
    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    create_summary_table(df_results)
    
    # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
    json_path = output_dir / "detailed_results.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    log_and_print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°JSON: {json_path}")
    
    # æœ€ç»ˆæ€»ç»“
    successful_models = len([r for r in all_results if r['status'] == 'success'])
    log_and_print("\n" + "="*80)
    log_and_print("ğŸ‰ YOLOæ¨¡å‹æ¯”è¾ƒå®éªŒå®Œæˆ!")
    log_and_print(f"âœ… æˆåŠŸè®­ç»ƒæ¨¡å‹æ•°: {successful_models}/{len(model_versions)}")
    log_and_print(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    log_and_print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_and_print("="*80) 