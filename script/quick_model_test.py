from ultralytics import YOLO
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import sys
from datetime import datetime

# è®¾ç½®ç®€å•çš„æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/quick_test.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = Path("quick_test_results")
output_dir.mkdir(exist_ok=True)

# è¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆåªé€‰æ‹©ç¡®å®šå¯ç”¨çš„ï¼‰
models_to_test = [
    "YOLOv7n", 
    "yolov6n",
    "yolov4n",
    "yolov3n",
    "yolov2n",
    "yolov1n"

    
]

def quick_test_model(model_name):
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹"""
    logger.info(f"ğŸš€ å¼€å§‹å¿«é€Ÿæµ‹è¯•æ¨¡å‹: {model_name}")
    
    try:
        # åŠ è½½å¹¶å¿«é€Ÿè®­ç»ƒæ¨¡å‹
        model = YOLO(f"./models/{model_name}.pt")
        logger.info(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
        
        # å¿«é€Ÿè®­ç»ƒï¼ˆå¾ˆå°‘çš„è½®æ•°ï¼‰
        train_results = model.train(
            data="label_data_dataset/dataset.yaml",
            epochs=10,  # åªè®­ç»ƒ10è½®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
            imgsz=640,
            device="6,7",
            batch=4,
            verbose=False,
            plots=False,
            project="quick_test_results",
            name=f"{model_name}_quick",
            exist_ok=True,
        )
        
        # éªŒè¯
        val_metrics = model.val(data="label_data.yaml", split='val', verbose=False)
        test_metrics = model.val(data="label_data.yaml", split='test', verbose=False)
        
        result = {
            'model': model_name,
            'val_mAP50': float(val_metrics.box.map50),
            'val_mAP50_95': float(val_metrics.box.map),
            'val_precision': float(val_metrics.box.mp),
            'val_recall': float(val_metrics.box.mr),
            'test_mAP50': float(test_metrics.box.map50),
            'test_mAP50_95': float(test_metrics.box.map),
            'test_precision': float(test_metrics.box.mp),
            'test_recall': float(test_metrics.box.mr),
            'status': 'success'
        }
        
        logger.info(f"ğŸ“Š {model_name} ç»“æœ - éªŒè¯mAP50: {val_metrics.box.map50:.4f}, æµ‹è¯•mAP50: {test_metrics.box.map50:.4f}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
        return {
            'model': model_name,
            'val_mAP50': 0, 'val_mAP50_95': 0, 'val_precision': 0, 'val_recall': 0,
            'test_mAP50': 0, 'test_mAP50_95': 0, 'test_precision': 0, 'test_recall': 0,
            'status': 'failed', 'error': str(e)
        }

def create_quick_chart(results_df):
    """åˆ›å»ºå¿«é€Ÿæ¯”è¾ƒå›¾è¡¨"""
    df_success = results_df[results_df['status'] == 'success']
    if len(df_success) == 0:
        logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœç”¨äºç»˜å›¾")
        return
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('YOLOæ¨¡å‹å¿«é€Ÿæ€§èƒ½æ¯”è¾ƒ', fontsize=16)
    
    models = df_success['model'].tolist()
    x = range(len(models))
    width = 0.35
    
    # mAP50
    ax1.bar([i-width/2 for i in x], df_success['val_mAP50'], width, label='éªŒè¯é›†', alpha=0.8)
    ax1.bar([i+width/2 for i in x], df_success['test_mAP50'], width, label='æµ‹è¯•é›†', alpha=0.8)
    ax1.set_title('mAP@0.5')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # mAP50-95
    ax2.bar([i-width/2 for i in x], df_success['val_mAP50_95'], width, label='éªŒè¯é›†', alpha=0.8)
    ax2.bar([i+width/2 for i in x], df_success['test_mAP50_95'], width, label='æµ‹è¯•é›†', alpha=0.8)
    ax2.set_title('mAP@0.5:0.95')
    ax2.set_xticks(x)
    ax2.set_xticklabels(models)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ç²¾ç¡®åº¦
    ax3.bar([i-width/2 for i in x], df_success['val_precision'], width, label='éªŒè¯é›†', alpha=0.8)
    ax3.bar([i+width/2 for i in x], df_success['test_precision'], width, label='æµ‹è¯•é›†', alpha=0.8)
    ax3.set_title('ç²¾ç¡®åº¦')
    ax3.set_xticks(x)
    ax3.set_xticklabels(models)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å¬å›ç‡
    ax4.bar([i-width/2 for i in x], df_success['val_recall'], width, label='éªŒè¯é›†', alpha=0.8)
    ax4.bar([i+width/2 for i in x], df_success['test_recall'], width, label='æµ‹è¯•é›†', alpha=0.8)
    ax4.set_title('å¬å›ç‡')
    ax4.set_xticks(x)
    ax4.set_xticklabels(models)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    chart_path = output_dir / "quick_comparison.png"
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    logger.info(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")

if __name__ == "__main__":
    logger.info(f"ğŸ å¼€å§‹å¿«é€Ÿæ¨¡å‹æµ‹è¯• - {datetime.now()}")
    logger.info(f"æµ‹è¯•æ¨¡å‹: {models_to_test}")
    
    all_results = []
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model_name in models_to_test:
        result = quick_test_model(model_name)
        all_results.append(result)
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(all_results)
    csv_path = output_dir / "quick_test_results.csv"
    df.to_csv(csv_path, index=False)
    logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
    
    # æ‰“å°ç»“æœè¡¨æ ¼
    logger.info("\n" + "="*80)
    logger.info("å¿«é€Ÿæµ‹è¯•ç»“æœæ±‡æ€»:")
    logger.info("="*80)
    for _, row in df.iterrows():
        if row['status'] == 'success':
            logger.info(f"{row['model']:<10} - éªŒè¯mAP50: {row['val_mAP50']:.4f}, æµ‹è¯•mAP50: {row['test_mAP50']:.4f}")
        else:
            logger.info(f"{row['model']:<10} - å¤±è´¥: {row.get('error', 'Unknown error')}")
    
    # åˆ›å»ºå›¾è¡¨
    create_quick_chart(df)
    
    logger.info(f"ğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆ - {datetime.now()}")
    logger.info(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    # æ˜¾ç¤ºæˆåŠŸçš„æ¨¡å‹æ•°é‡
    success_count = len(df[df['status'] == 'success'])
    logger.info(f"âœ… æˆåŠŸæµ‹è¯•æ¨¡å‹æ•°: {success_count}/{len(models_to_test)}") 